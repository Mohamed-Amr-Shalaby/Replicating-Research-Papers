{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epec2Ef9RcsG"
      },
      "source": [
        "## Notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYx7qJvdRcsH"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Notes from the paper:\n",
        "\n",
        "The Alexnet paper used Convolutional Neural Networks to win the ImageNet competition in 2012.\n",
        "\n",
        "Goal:\n",
        "Image Classification\n",
        "\n",
        "Dataset Used:\n",
        "Imagenet-1000\n",
        "Imagenet is a 15 million labelled high-resolution (Relatively speeaking, compared to NIST which was 28 x28, this is 256 x 256) images in 22,000 categories.\n",
        "The 1000 category subset was used for this paper. which is over 1.2 Million images lol. That's ALOT. This might be the biggest dataset I have ever worked with.\n",
        "\n",
        "Method Used:\n",
        "Convolution layers, occasionally followed by max-pooling layers. The final layers are fully connected layers, with Dropout layers in between.\n",
        "Ends with a 1000-way softmax layer.\n",
        "\n",
        "Convolution dimension calculation:\n",
        "https://madebyollin.github.io/convnet-calculator/\n",
        "\n",
        "Architecture:\n",
        "The input is originally 256 x 256, but is cropped to 224 x 224, and then fed into the network (data augmentation).\n",
        "Input (3 x 224, 224)\n",
        "DONE\n",
        "-------------------------------------------------------------------------------------------------------------------------------------------\n",
        "- Convolutional Layer 1\n",
        "    GPU1 - (96 filters, 11 x 11, stride 4, padding 0) -> (output dim: (96, 55, 55))\n",
        "    GPU2 - (96 filters, 11 x 11, stride 4, padding 0) -> (output dim: (96, 55, 55))\n",
        "- Max Pooling Layer 1\n",
        "    GPU1 - (3 x 3, stride 2) -> (output dim: (96, 27, 27))\n",
        "    GPU2 - (3 x 3, stride 2) -> (output dim: (96, 27, 27))\n",
        "- Convolutional Layer 2\n",
        "    GPU1 - (256 filters, 5 x 5, stride 1, padding 2) -> (output dim: (256, 27, 27))\n",
        "    GPU2 - (256 filters, 5 x 5, stride 1, padding 2) -> (output dim: (256, 27, 27))\n",
        "- Max Pooling Layer 2\n",
        "    GPU1 - (3 x 3, stride 2) -> (output dim: (256, 13, 13))\n",
        "    GPU2 - (3 x 3, stride 2) -> (output dim: (256, 13, 13))\n",
        "- Convolutional Layer 3\n",
        "    GPU1 - (384 filters, 3 x 3, stride 1, padding 1) -> (output dim: (384, 13, 13))\n",
        "    GPU2 - (384 filters, 3 x 3, stride 1, padding 1) -> (output dim: (384, 13, 13))\n",
        "- Convolutional Layer 4\n",
        "    GPU1 - (384 filters, 3 x 3, stride 1, padding 1) -> (output dim: (384, 13, 13))\n",
        "    GPU2 - (384 filters, 3 x 3, stride 1, padding 1) -> (output dim: (384, 13, 13))\n",
        "- Convolutional Layer 5\n",
        "    GPU1 - (256 filters, 3 x 3, stride 1, padding 1) -> (output dim: (256, 13, 13))\n",
        "    GPU2 - (256 filters, 3 x 3, stride 1, padding 1) -> (output dim: (256, 13, 13))\n",
        "- Max Pooling Layer 3\n",
        "    GPU1 - (3 x 3, stride 2) -> (output dim: (256, 6, 6))\n",
        "    GPU2 - (3 x 3, stride 2) -> (output dim: (256, 6, 6)) # Flattened this becomes 9216 neurons\n",
        "- Dropout Layer 1\n",
        "    Dropout 0.5\n",
        "- Fully Connected Layer 1\n",
        "    GPU1 - (9216 neurons) -> (output dim: (4096))\n",
        "    GPU2 - (9216 neurons) -> (output dim: (4096))\n",
        "- Fully Connected Layer 2\n",
        "    GPU1 - (4096 neurons) -> (output dim: (4096))\n",
        "    GPU2 - (4096 neurons) -> (output dim: (4096))\n",
        "- Fully Connected Layer 3\n",
        "    4096 Neurons -> (output dim: (1000))\n",
        "- Softmax Layer\n",
        "    1000 Neurons -> (output dim: (1000))\n",
        "\n",
        "Keep in mind this:\n",
        "\n",
        "```\n",
        "Now we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net\n",
        "contains eight layers with weights; the first five are convolutional and the remaining three are fullyconnected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces\n",
        "a distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression\n",
        "objective, which is equivalent to maximizing the average across training cases of the log-probability\n",
        "of the correct label under the prediction distribution.\n",
        "The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel\n",
        "maps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third\n",
        "convolutional layer are connected to all kernel maps in the second layer. The neurons in the fullyconnected layers are connected to all neurons in the previous layer. Response-normalization layers\n",
        "follow the first and second convolutional layers. Max-pooling layers, of the kind described in Section\n",
        "3.4, follow both response-normalization layers as well as the fifth convolutional layer. The ReLU\n",
        "non-linearity is applied to the output of every convolutional and fully-connected layer\n",
        "```\n",
        "\n",
        "Training Parameters / Hyperparamters:\n",
        "- Data Augmentation: Randomly cropped 224x224 patches from the 256x256 images, and horizontally mirroring them.\n",
        "    - This means that on test time, the image is resized to 256x256, and then 5 224x224 patches are cropped from it, and mirrored, and the network is run on all of them. The final prediction is the average of the 10 predictions.\n",
        "- They wrote a Cuda ConvNet from scratch to train the network. BASED\n",
        "- SGD with momentum 0.9 and weight decay 0.0005\n",
        "- Batch Size: 128\n",
        "\n",
        "Metrics Defined:\n",
        "Error Rate\n",
        "- Number of misclassified test samples / Total number of test samples\n",
        "\n",
        "Top 1 vs top 5 error rate\n",
        "- Top 1 error rate is the number of test samples for which the correct label is not among the top 1 predicted labels\n",
        "- Top 5 error rate is the number of test samples for which the correct label is not among the top 5 predicted labels\n",
        "\n",
        "Results:\n",
        "- Top-1 error rate: 37.5%\n",
        "- Top-5 error rate: 17.0%\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"http://aisdatasets.informatik.uni-freiburg.de/freiburg_groceries_dataset/freiburg_groceries_dataset.tar.gz\""
      ],
      "metadata": {
        "id": "Avgx6CYRReTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf \"freiburg_groceries_dataset.tar.gz\""
      ],
      "metadata": {
        "id": "WE-0wfhgRiNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import image paths in a way where we have a dataframe with the structure --> Path|Folder_name (aka label)\n",
        "import glob\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8eRQtD_5Rm6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glob.glob(\"images/*/*.png\")"
      ],
      "metadata": {
        "id": "xdOTfBYQVSqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = dict()\n",
        "# Dictionary where structure is {Folder name: [Image1, Image2,...]}\n",
        "\n",
        "folder_names = glob.glob(\"images/*\")"
      ],
      "metadata": {
        "id": "Q2V-XmpfVYfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = glob.glob(\"images/*/*\")\n",
        "labels = pd.Series(image_paths)"
      ],
      "metadata": {
        "id": "4pQSrRAWWXoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = labels.str.split(pat=\"/\", expand=True)"
      ],
      "metadata": {
        "id": "20ZkfnNrWYNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "id": "3aYL1cHKXVIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = pd.DataFrame(image_paths)\n",
        "image_paths"
      ],
      "metadata": {
        "id": "Yta4vIm2XzoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_dict = dict()\n",
        "images_dict[\"image_paths\"] = image_paths[0].values\n",
        "images_dict[\"labels\"] = labels[1].values\n",
        "images_dict"
      ],
      "metadata": {
        "id": "0fgCTLBNZdHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.DataFrame(images_dict)"
      ],
      "metadata": {
        "id": "at6K6_AlZq68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "D4jXZ1VeZ6UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "l2yBREQeatLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch imports\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose, v2\n",
        "from torch.nn.functional import one_hot\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "OvgSxACTbL1V"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class groceries_dataset_class(Dataset):\n",
        "  def __init__(self, dataframe, transform = None):\n",
        "    self.dataframe = dataframe\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = torchvision.io.read_image(self.dataframe[\"image_paths\"][idx])\n",
        "    label = self.dataframe[\"labels\"][idx]\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image, label\n"
      ],
      "metadata": {
        "id": "gaNbxyF6Z6xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "transforms = Compose([\n",
        "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "groceries_dataset = groceries_dataset_class(dataset, transform=transforms)\n",
        "train_loader = DataLoader(groceries_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(groceries_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "xc2BfZ5OcbHB"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test dataset\n",
        "image, label = groceries_dataset[3]"
      ],
      "metadata": {
        "id": "5fo-sNdYc3Gl"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNL_27UpdtEI",
        "outputId": "3caedec0-5c7a-4272-cbba-04132ec39554"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN Model, Hyperparameters"
      ],
      "metadata": {
        "id": "sD0MojWLp4z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "9UJwPiG9p9UR"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AlexNet).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0) # 96, 55, 55\n",
        "    self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2) # 256, 27, 27\n",
        "    self.conv3 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv5 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n",
        "    self.drop = nn.Dropout(p=0.5)\n",
        "    self.fc6 = nn.Linear(6*6*256, 4096)\n",
        "    self.fc7 = nn.Linear(4096, 4096)\n",
        "    self.fc8 = nn.Linear(4096, 1000)\n",
        "    self.sm = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = F.relu(self.conv5(x))\n",
        "    x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "    x = x.view(-1, 256*6*6)\n",
        "    x = F.relu(self.fc6(x))\n",
        "    x = self.drop(x)\n",
        "    x = F.relu(self.fc7(x))\n",
        "    x = self.drop(x)\n",
        "    x = F.relu(self.fc8(x))\n",
        "    x = self.sm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "DbIeIx7meotd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize model and test it\n",
        "model = AlexNet().cuda()\n",
        "image = torch.randn(1, 3, 224, 224)\n",
        "output = model(image.cuda()).cpu()\n",
        "output.size()"
      ],
      "metadata": {
        "id": "-60SPMgxq2Qj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}