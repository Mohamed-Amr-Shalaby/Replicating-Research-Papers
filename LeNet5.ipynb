{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating LeNet 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notes from the paper:\n",
    "\n",
    "The Lenet paper summrizes previous work done on character recognition, including SGD, Convolutions and Neural Networks.\n",
    "\n",
    "Goal:\n",
    "Character Recognition, by building a character classifier\n",
    "\n",
    "Dataset Used:\n",
    "MNIST\n",
    "\n",
    "Method Used:\n",
    "Build a Convolution based Feature Extractor, followed by a Fully Connected Neural Network Classifier\n",
    "\n",
    "Architecture:\n",
    "Input (32, 32)\n",
    "-> Convolution (5x5, 6 filters) (6, 28, 28)\n",
    "-> Sub Sampling (6, 14, 14)\n",
    "-> Sigmoid\n",
    "-> Convolution (5x5, 16 filters) (16, 10, 10)\n",
    "-> Sub Sampling (16, 5, 5)\n",
    "-> Sigmoid\n",
    "-> Convolution (5x5, 120 filters) (120, 1, 1)\n",
    "-> Sigmoid\n",
    "-> Fully Connected (120)\n",
    "-> Sigmoid\n",
    "-> Fully Connected (84)\n",
    "-> Sigmoid\n",
    "-> RBF (10)\n",
    "\n",
    "Training Parameters / Hyperparamters:\n",
    "- Important to note detail is the the dataset is 28 x 28. Padding is added to the image to better extract stroke-endpoints on the edges on the images\n",
    "- Image is norrmalized to have zero mean and equal variance.\n",
    "- Sumsampling means, in a 2x2 pixel area, all values are arred, multiplied by a weight and added to a bias. This IS NOT THE SAME AS MAX POOLING.\n",
    "- Stride for subsampling is 2, so that the output is half the size of the input and the area of sub-sampling is non overlaping\n",
    "- S2 and C3 have some weird associations which I will ignore probably\n",
    "- The last layer is a layer of RBF units instead of neurons. The Paper explains, \"In probabilistic terms, the RBF output can be interpreted as the unnormalized negative loglikelihood of a Gaussian distribution in the space of configurations of layer F6\"\n",
    "- Loss function is MSE, but they modify it and make it scary. We will just just MSE loss\n",
    "\n",
    "- Ran three Experiments\n",
    "- 1. Images were centered into a 28 x 28 image and then padded to 32 x 32. This was called the \"Regular\" dataset\n",
    "- 2. Images were deslanted and cropped into a 20 x 20 image. This was called the \"Deslanted\" dataset\n",
    "- 3. Images were centered into a 16 x 16 image. The Author forgot to name this dataset like it was his middle child.\n",
    "\n",
    "I will only be using the Regular Dataset.\n",
    "\n",
    "- Trained for 20 epochs\n",
    "- 60k training images, 10k test images\n",
    "- Learning Rate was 0.0005 for the first 2 epochs, and 0.0002 for the next 3, 0.0001 fir the next 3, 0.00005 for the next 4 and 0.00001 thereafter.\n",
    "- Author obeserver no over-fitting? Is he Jesus? The Author says this is because the learning rates are too high? LMFAO\n",
    "- \n",
    "\n",
    "Metrics Defined:\n",
    "Error Rate\n",
    "- Number of misclassified test samples / Total number of test samples\n",
    "\n",
    "Results:\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
