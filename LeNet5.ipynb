{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2uXZOEpX7pu"
      },
      "source": [
        "# Replicating LeNet 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjjUmz4EX7pu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Notes from the paper:\n",
        "\n",
        "The Lenet paper summrizes previous work done on character recognition, including SGD, Convolutions and Neural Networks.\n",
        "\n",
        "Goal:\n",
        "Character Recognition, by building a character classifier\n",
        "\n",
        "Dataset Used:\n",
        "MNIST\n",
        "\n",
        "Method Used:\n",
        "Build a Convolution based Feature Extractor, followed by a Fully Connected Neural Network Classifier\n",
        "\n",
        "Architecture:\n",
        "Input (32, 32)\n",
        "-> Convolution (5x5, 6 filters) (6, 28, 28)\n",
        "-> Sub Sampling (6, 14, 14)\n",
        "-> Sigmoid\n",
        "-> Convolution (5x5, 16 filters) (16, 10, 10)\n",
        "-> Sub Sampling (16, 5, 5)\n",
        "-> Sigmoid\n",
        "-> Convolution (5x5, 120 filters) (120, 1, 1)\n",
        "-> Sigmoid\n",
        "-> Fully Connected (120)\n",
        "-> Sigmoid\n",
        "-> Fully Connected (84)\n",
        "-> Sigmoid\n",
        "-> RBF (10)\n",
        "\n",
        "Training Parameters / Hyperparamters:\n",
        "- Important to note detail is the the dataset is 28 x 28. Padding is added to the image to better extract stroke-endpoints on the edges on the images\n",
        "- Image is norrmalized to have zero mean and equal variance.\n",
        "- Sumsampling means, in a 2x2 pixel area, all values are arred, multiplied by a weight and added to a bias. This IS NOT THE SAME AS MAX POOLING.\n",
        "- Stride for subsampling is 2, so that the output is half the size of the input and the area of sub-sampling is non overlaping\n",
        "- S2 and C3 have some weird associations which I will ignore probably\n",
        "- The last layer is a layer of RBF units instead of neurons. The Paper explains, \"In probabilistic terms, the RBF output can be interpreted as the unnormalized negative loglikelihood of a Gaussian distribution in the space of configurations of layer F6\"\n",
        "- Loss function is MSE, but they modify it and make it scary. We will just just MSE loss\n",
        "\n",
        "- Ran three Experiments\n",
        "- 1. Images were centered into a 28 x 28 image and then padded to 32 x 32. This was called the \"Regular\" dataset\n",
        "- 2. Images were deslanted and cropped into a 20 x 20 image. This was called the \"Deslanted\" dataset\n",
        "- 3. Images were centered into a 16 x 16 image. The Author forgot to name this dataset like it was his middle child.\n",
        "\n",
        "I will only be using the Regular Dataset.\n",
        "\n",
        "- Trained for 20 epochs\n",
        "- 60k training images, 10k test images\n",
        "- Learning Rate was 0.0005 for the first 2 epochs, and 0.0002 for the next 3, 0.0001 fir the next 3, 0.00005 for the next 4 and 0.00001 thereafter.\n",
        "- Author obeserver no over-fitting? Is he Jesus? The Author says this is because the learning rates are too high? LMFAO\n",
        "-\n",
        "\n",
        "Metrics Defined:\n",
        "Error Rate\n",
        "- Number of misclassified test samples / Total number of test samples\n",
        "\n",
        "Results:\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "ohU4q18_X9nn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class sub_sampler(nn.Module):\n",
        "  def __init__(self, size, stride):\n",
        "    super(sub_sampler, self).__init__()\n",
        "    self.size = size\n",
        "    self.stride = stride\n",
        "    self.pool = nn.AvgPool2d(self.size, self.stride)\n",
        "    self.weight = nn.Parameter(torch.ones(1))\n",
        "    self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self, img):\n",
        "    img = self.pool(img)\n",
        "    return img * self.weight + self.bias"
      ],
      "metadata": {
        "id": "6eDpMLThX_nr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RBFLayer(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(RBFLayer, self).__init__()\n",
        "    self.centers = nn.Parameter(torch.randn(output_dim, input_dim)) # This creates a centers array with dimensions (output_dim, input_dim) and randomizes it. It's a parameter because centers are trainable\n",
        "\n",
        "  def forward(self, x): # X will be passed in batches so keep that in mind\n",
        "    distances = torch.cdist(torch.unsqueeze(x, 1), torch.unsqueeze(self.centers, 0)) # The x shape will be (batch_size, 1, input_dim) and centers shape will be (1, output_dim, input_dim)\n",
        "    # Distances output dimensions will be (batch_size, 1, output_dim)\n",
        "    return torch.exp(-1.0 * distances.squeeze(1))\n"
      ],
      "metadata": {
        "id": "1XmuNZwgZUjt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet5, self).__init__()\n",
        "    self.c1 = nn.Conv2d(1, 6, 5, 1, 2) # (1, 28, 28) -> (1, 32, 32) -> (6, 28, 28)\n",
        "    self.s2 = sub_sampler(2, 2) # (6, 14, 14)\n",
        "    self.c3 = nn.Conv2d(6, 16, 5, stride=1, padding=0) # (16, 10, 10)\n",
        "    self.s4 = sub_sampler(2, 2) # (16, 5, 5)\n",
        "    self.c5 = nn.Conv2d(16, 120, 5, stride=1, padding=0) # (120, 1)\n",
        "    self.f6 = nn.Linear(120, 84)\n",
        "    self.rbf = RBFLayer(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.sigmoid(self.s2(self.c1(x)))\n",
        "    x = self.s4(self.c3(x))\n",
        "    x = self.c5(x)\n",
        "    x = x.view(-1, 120)\n",
        "    x = self.f6(x)\n",
        "    x = self.rbf(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Eh7mfanhdQKJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing model class\n",
        "model = LeNet5()\n",
        "image = torch.randn(5, 1, 28, 28)\n",
        "output = model(image)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "n9q2pit4sK6a",
        "outputId": "35dc4b72-ebb2-409c-d042-f0b428709682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "from torch.nn.functional import one_hot\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "vdt3r7Tpx82J"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = MNIST('./data', download=True, train=True)\n",
        "test_data = MNIST('./data', download=True, train=False)"
      ],
      "metadata": {
        "id": "det6j7aazdRX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataset(nn.Module):\n",
        "  def __init__(self, dataset, transform = None):\n",
        "    super(MNISTDataset, self).__init__()\n",
        "    self.dataset = dataset\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image, label = self.dataset[idx]\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image, one_hot(torch.tensor(label), num_classes = 10).float()\n"
      ],
      "metadata": {
        "id": "0O_Uof45rtOd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hard_coded_normalize(img, min = -0.1, max = 1.175) -> torch.Tensor:\n",
        "  img = img.float()\n",
        "  img = img * (max-min) / 1 + min # Standardization\n",
        "  return img\n",
        "\n",
        "transforms = Compose([\n",
        "    ToTensor,\n",
        "    Lambda(hard_coded_normalize)\n",
        "])\n",
        "\n",
        "mnist_train_dataset = MNISTDataset(train_data, transforms)\n",
        "mnist_test_dataset = MNISTDataset(test_data, transforms)"
      ],
      "metadata": {
        "id": "IEdOhIiBz2ZL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "wwvJKde40p8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_loader = DataLoader(mnist_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test_dataset, batch_size=batch_size, shuffle=False)\n",
        "lr = 0.01"
      ],
      "metadata": {
        "id": "37r_iRQ30ljF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import SGD"
      ],
      "metadata": {
        "id": "YlBArkE91M_F"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LeNet5().cuda()\n",
        "epochs = 20\n",
        "optimizer = SGD(model.parameters(), lr=lr)\n",
        "loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Kmy5-x331OfG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}