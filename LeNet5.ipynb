{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamed-Amr-Shalaby/Replicating-Research-Papers/blob/main/LeNet5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2uXZOEpX7pu"
      },
      "source": [
        "# Replicating LeNet 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XjjUmz4EX7pu",
        "outputId": "46f7afdc-f04d-4be7-fc96-cf989ddc6151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNotes from the paper:\\n\\nThe Lenet paper summrizes previous work done on character recognition, including SGD, Convolutions and Neural Networks.\\n\\nGoal:\\nCharacter Recognition, by building a character classifier\\n\\nDataset Used:\\nMNIST\\n\\nMethod Used:\\nBuild a Convolution based Feature Extractor, followed by a Fully Connected Neural Network Classifier\\n\\nArchitecture:\\nInput (32, 32)\\n-> Convolution (5x5, 6 filters) (6, 28, 28)\\n-> Sub Sampling (6, 14, 14)\\n-> Sigmoid\\n-> Convolution (5x5, 16 filters) (16, 10, 10)\\n-> Sub Sampling (16, 5, 5)\\n-> Sigmoid\\n-> Convolution (5x5, 120 filters) (120, 1, 1)\\n-> Sigmoid\\n-> Fully Connected (120)\\n-> Sigmoid\\n-> Fully Connected (84)\\n-> Sigmoid\\n-> RBF (10)\\n\\nTraining Parameters / Hyperparamters:\\n- Important to note detail is the the dataset is 28 x 28. Padding is added to the image to better extract stroke-endpoints on the edges on the images\\n- Image is norrmalized to have zero mean and equal variance.\\n- Sumsampling means, in a 2x2 pixel area, all values are arred, multiplied by a weight and added to a bias. This IS NOT THE SAME AS MAX POOLING.\\n- Stride for subsampling is 2, so that the output is half the size of the input and the area of sub-sampling is non overlaping\\n- S2 and C3 have some weird associations which I will ignore probably\\n- The last layer is a layer of RBF units instead of neurons. The Paper explains, \"In probabilistic terms, the RBF output can be interpreted as the unnormalized negative loglikelihood of a Gaussian distribution in the space of configurations of layer F6\"\\n- Loss function is MSE, but they modify it and make it scary. We will just just MSE loss\\n\\n- Ran three Experiments\\n- 1. Images were centered into a 28 x 28 image and then padded to 32 x 32. This was called the \"Regular\" dataset\\n- 2. Images were deslanted and cropped into a 20 x 20 image. This was called the \"Deslanted\" dataset\\n- 3. Images were centered into a 16 x 16 image. The Author forgot to name this dataset like it was his middle child.\\n\\nI will only be using the Regular Dataset.\\n\\n- Trained for 20 epochs\\n- 60k training images, 10k test images\\n- Learning Rate was 0.0005 for the first 2 epochs, and 0.0002 for the next 3, 0.0001 fir the next 3, 0.00005 for the next 4 and 0.00001 thereafter.\\n- Author obeserver no over-fitting? Is he Jesus? The Author says this is because the learning rates are too high? LMFAO\\n-\\n\\nMetrics Defined:\\nError Rate\\n- Number of misclassified test samples / Total number of test samples\\n\\nResults:\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "Notes from the paper:\n",
        "\n",
        "The Lenet paper summrizes previous work done on character recognition, including SGD, Convolutions and Neural Networks.\n",
        "\n",
        "Goal:\n",
        "Character Recognition, by building a character classifier\n",
        "\n",
        "Dataset Used:\n",
        "MNIST\n",
        "\n",
        "Method Used:\n",
        "Build a Convolution based Feature Extractor, followed by a Fully Connected Neural Network Classifier\n",
        "\n",
        "Architecture:\n",
        "Input (32, 32)\n",
        "-> Convolution (5x5, 6 filters) (6, 28, 28)\n",
        "-> Sub Sampling (6, 14, 14)\n",
        "-> Sigmoid\n",
        "-> Convolution (5x5, 16 filters) (16, 10, 10)\n",
        "-> Sub Sampling (16, 5, 5)\n",
        "-> Sigmoid\n",
        "-> Convolution (5x5, 120 filters) (120, 1, 1)\n",
        "-> Sigmoid\n",
        "-> Fully Connected (120)\n",
        "-> Sigmoid\n",
        "-> Fully Connected (84)\n",
        "-> Sigmoid\n",
        "-> RBF (10)\n",
        "\n",
        "Training Parameters / Hyperparamters:\n",
        "- Important to note detail is the the dataset is 28 x 28. Padding is added to the image to better extract stroke-endpoints on the edges on the images\n",
        "- Image is norrmalized to have zero mean and equal variance.\n",
        "- Sumsampling means, in a 2x2 pixel area, all values are arred, multiplied by a weight and added to a bias. This IS NOT THE SAME AS MAX POOLING.\n",
        "- Stride for subsampling is 2, so that the output is half the size of the input and the area of sub-sampling is non overlaping\n",
        "- S2 and C3 have some weird associations which I will ignore probably\n",
        "- The last layer is a layer of RBF units instead of neurons. The Paper explains, \"In probabilistic terms, the RBF output can be interpreted as the unnormalized negative loglikelihood of a Gaussian distribution in the space of configurations of layer F6\"\n",
        "- Loss function is MSE, but they modify it and make it scary. We will just just MSE loss\n",
        "\n",
        "- Ran three Experiments\n",
        "- 1. Images were centered into a 28 x 28 image and then padded to 32 x 32. This was called the \"Regular\" dataset\n",
        "- 2. Images were deslanted and cropped into a 20 x 20 image. This was called the \"Deslanted\" dataset\n",
        "- 3. Images were centered into a 16 x 16 image. The Author forgot to name this dataset like it was his middle child.\n",
        "\n",
        "I will only be using the Regular Dataset.\n",
        "\n",
        "- Trained for 20 epochs\n",
        "- 60k training images, 10k test images\n",
        "- Learning Rate was 0.0005 for the first 2 epochs, and 0.0002 for the next 3, 0.0001 fir the next 3, 0.00005 for the next 4 and 0.00001 thereafter.\n",
        "- Author obeserver no over-fitting? Is he Jesus? The Author says this is because the learning rates are too high? LMFAO\n",
        "-\n",
        "\n",
        "Metrics Defined:\n",
        "Error Rate\n",
        "- Number of misclassified test samples / Total number of test samples\n",
        "\n",
        "Results:\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "ohU4q18_X9nn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class sub_sampler(nn.Module):\n",
        "  def __init__(self, size, stride):\n",
        "    super(sub_sampler, self).__init__()\n",
        "    self.size = size\n",
        "    self.stride = stride\n",
        "    self.pool = nn.AvgPool2d(self.size, self.stride)\n",
        "    self.weight = nn.Parameter(torch.ones(1))\n",
        "    self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self, img):\n",
        "    img = self.pool(img)\n",
        "    return img * self.weight + self.bias"
      ],
      "metadata": {
        "id": "6eDpMLThX_nr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RBFLayer(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(RBFLayer, self).__init__()\n",
        "    self.centers = nn.Parameter(torch.randn(output_dim, input_dim)) # This creates a centers array with dimensions (output_dim, input_dim) and randomizes it. It's a parameter because centers are trainable\n",
        "\n",
        "  def forward(self, x): # X will be passed in batches so keep that in mind\n",
        "    distances = torch.cdist(torch.unsqueeze(x, 1), torch.unsqueeze(self.centers, 0)) # The x shape will be (batch_size, 1, input_dim) and centers shape will be (1, output_dim, input_dim)\n",
        "    # Distances output dimensions will be (batch_size, 1, output_dim)\n",
        "    return torch.exp(-1.0 * distances.squeeze(1))\n"
      ],
      "metadata": {
        "id": "1XmuNZwgZUjt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet5, self).__init__()\n",
        "    self.c1 = nn.Conv2d(1, 6, 5, 1, 2) # (1, 28, 28) -> (1, 32, 32) -> (6, 28, 28)\n",
        "    self.s2 = sub_sampler(2, stride=2) # (6, 14, 14)\n",
        "    self.c3 = nn.Conv2d(6, 16, 5, stride=1, padding=0) # (16, 10, 10)\n",
        "    self.s4 = sub_sampler(2, stride=2) # (16, 5, 5)\n",
        "    self.c5 = nn.Conv2d(16, 120, 5, stride=1, padding=0) # (120, 1)\n",
        "    self.f6 = nn.Linear(120, 84)\n",
        "    self.rbf = RBFLayer(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.sigmoid(self.s2(self.c1(x)))\n",
        "    x = self.s4(self.c3(x))\n",
        "    x = self.c5(x)\n",
        "    x = x.view(-1, 120)\n",
        "    x = self.f6(x)\n",
        "    x = self.rbf(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Eh7mfanhdQKJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing model class\n",
        "model = LeNet5()\n",
        "image = torch.randn(5, 1, 28, 28)\n",
        "output = model(image)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "n9q2pit4sK6a",
        "outputId": "8e8d0f3a-8a17-4251-941c-60104911f02d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "from torch.nn.functional import one_hot\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "vdt3r7Tpx82J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = MNIST('./data', download=True, train=True)\n",
        "test_data = MNIST('./data', download=True, train=False)"
      ],
      "metadata": {
        "id": "det6j7aazdRX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataset(nn.Module):\n",
        "  def __init__(self, dataset, transform = None):\n",
        "    super(MNISTDataset, self).__init__()\n",
        "    self.dataset = dataset\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image, label = self.dataset[idx]\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "0O_Uof45rtOd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transforms = Compose([ToTensor()])\n",
        "\n",
        "mnist_train_dataset = MNISTDataset(train_data, transforms)\n",
        "mnist_test_dataset = MNISTDataset(test_data, transforms)"
      ],
      "metadata": {
        "id": "IEdOhIiBz2ZL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "wwvJKde40p8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_loader = DataLoader(mnist_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test_dataset, batch_size=batch_size, shuffle=False)\n",
        "lr = 0.01"
      ],
      "metadata": {
        "id": "37r_iRQ30ljF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import SGD"
      ],
      "metadata": {
        "id": "YlBArkE91M_F"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LeNet5().cuda()\n",
        "epochs = 20\n",
        "optimizer = SGD(model.parameters(), lr=lr)\n",
        "loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Kmy5-x331OfG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y_true):\n",
        "  # Since neither y_pred or y_true is onehot encoded (due to CrossEntropyLoss), we find the highest number predicted on the output layer, and that's our prediction\n",
        "  y_pred_class = y_pred.argmax(dim=1)\n",
        "  return (y_true == y_pred_class).float().mean()\n"
      ],
      "metadata": {
        "id": "E-SFFiM3mR1J"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training loop\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  train_loss_val = 0\n",
        "  train_acc_val = 0\n",
        "\n",
        "  # Train dataset\n",
        "  for images, labels in train_loader:\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    train_loss = loss(outputs, labels)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss_val += train_loss.item()\n",
        "    train_acc_val += accuracy(outputs, labels).item()\n",
        "\n",
        "  train_loss_val /= len (train_loader)\n",
        "  train_acc_val /= len(train_loader)\n",
        "\n",
        "  # Evaluation\n",
        "  model.eval()\n",
        "  test_loss_val = 0\n",
        "  test_acc_val = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      outputs = model(images)\n",
        "      test_loss_val += loss(outputs, labels).item()\n",
        "      test_acc_val += accuracy(outputs, labels).item()\n",
        "\n",
        "    test_loss_val /= len(test_loader)\n",
        "    test_acc_val /= len(test_loader)\n",
        "\n",
        "  print(f\"Epoch {epoch + 1}: Test Loss = {test_loss_val:.6f}, Train Loss = {train_loss_val:.6f}\")\n",
        "  print(f\"Epoch {epoch + 1}: Test Accuracy = {test_acc_val:.4f}, Train Accuracy = {train_acc_val:.4f}\")\n"
      ],
      "metadata": {
        "id": "0R0M-UyJSjg6",
        "outputId": "a78d5758-ba74-4330-a001-b9b68af47c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Test Loss = 2.302587, Train Loss = 2.302586\n",
            "Epoch 1: Test Accuracy = 0.1026, Train Accuracy = 0.1044\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a5b57204cc27>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Got it, shitty result, probably RBF, let's redefine the Model"
      ],
      "metadata": {
        "id": "c6tIIAEAfHcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(LeNet5, self).__init__()\n",
        "      self.c1 = nn.Conv2d(1, 6, 5, 1, 2) # (1, 28, 28) -> (1, 32, 32) -> (6, 28, 28)\n",
        "      self.s2 = sub_sampler(2, stride=2) # (6, 14, 14)\n",
        "      self.c3 = nn.Conv2d(6, 16, 5, stride=1, padding=0) # (16, 10, 10)\n",
        "      self.s4 = sub_sampler(2, stride=2) # (16, 5, 5)\n",
        "      self.c5 = nn.Conv2d(16, 120, 5, stride=1, padding=0) # (120, 1)\n",
        "      self.f6 = nn.Linear(120, 84)\n",
        "      self.f7 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.sigmoid(self.s2(self.c1(x)))\n",
        "      x = self.s4(self.c3(x))\n",
        "      x = self.c5(x)\n",
        "      x = x.view(-1, 120)\n",
        "      x = self.f6(x)\n",
        "      x = self.f7(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "P73XTEEPc_XC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  train_loss_val = 0\n",
        "  train_acc_val = 0\n",
        "\n",
        "  # Train dataset\n",
        "  for images, labels in train_loader:\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    train_loss = loss(outputs, labels)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss_val += train_loss.item()\n",
        "    train_acc_val += accuracy(outputs, labels).item()\n",
        "\n",
        "  train_loss_val /= len(train_loader)\n",
        "  train_acc_val /= len(train_loader)\n",
        "\n",
        "  # Evaluation\n",
        "  model.eval()\n",
        "  test_loss_val = 0\n",
        "  test_acc_val = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      outputs = model(images)\n",
        "      test_loss_val += loss(outputs, labels).item()\n",
        "      test_acc_val += accuracy(outputs, labels).item()\n",
        "\n",
        "  test_loss_val /= len(test_loader)\n",
        "  test_acc_val /= len(test_loader)\n",
        "\n",
        "  print(f\"Epoch {epoch + 1}: Test Loss = {test_loss_val:.6f}, Train Loss = {train_loss_val:.6f}\")\n",
        "  print(f\"Epoch {epoch + 1}: Test Accuracy = {test_acc_val:.4f}, Train Accuracy = {train_acc_val:.4f}\")"
      ],
      "metadata": {
        "id": "xk86m_0ZfVRS",
        "outputId": "70aca071-2172-48b5-b624-c2f6ebc17ba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Test Loss = 2.272813, Train Loss = 2.292783\n",
            "Epoch 1: Test Accuracy = 0.1162, Train Accuracy = 0.1289\n",
            "Epoch 2: Test Loss = 0.515958, Train Loss = 1.479511\n",
            "Epoch 2: Test Accuracy = 0.8403, Train Accuracy = 0.5620\n",
            "Epoch 3: Test Loss = 0.373712, Train Loss = 0.436185\n",
            "Epoch 3: Test Accuracy = 0.8853, Train Accuracy = 0.8670\n",
            "Epoch 4: Test Loss = 0.343281, Train Loss = 0.368429\n",
            "Epoch 4: Test Accuracy = 0.8974, Train Accuracy = 0.8895\n",
            "Epoch 5: Test Loss = 0.313940, Train Loss = 0.344816\n",
            "Epoch 5: Test Accuracy = 0.9081, Train Accuracy = 0.8963\n",
            "Epoch 6: Test Loss = 0.299104, Train Loss = 0.325528\n",
            "Epoch 6: Test Accuracy = 0.9107, Train Accuracy = 0.9028\n",
            "Epoch 7: Test Loss = 0.291185, Train Loss = 0.306646\n",
            "Epoch 7: Test Accuracy = 0.9136, Train Accuracy = 0.9087\n",
            "Epoch 8: Test Loss = 0.250689, Train Loss = 0.284410\n",
            "Epoch 8: Test Accuracy = 0.9264, Train Accuracy = 0.9157\n",
            "Epoch 9: Test Loss = 0.244037, Train Loss = 0.255608\n",
            "Epoch 9: Test Accuracy = 0.9304, Train Accuracy = 0.9243\n",
            "Epoch 10: Test Loss = 0.182925, Train Loss = 0.219731\n",
            "Epoch 10: Test Accuracy = 0.9476, Train Accuracy = 0.9362\n",
            "Epoch 11: Test Loss = 0.179433, Train Loss = 0.185327\n",
            "Epoch 11: Test Accuracy = 0.9459, Train Accuracy = 0.9455\n",
            "Epoch 12: Test Loss = 0.134699, Train Loss = 0.158922\n",
            "Epoch 12: Test Accuracy = 0.9604, Train Accuracy = 0.9534\n",
            "Epoch 13: Test Loss = 0.129758, Train Loss = 0.142473\n",
            "Epoch 13: Test Accuracy = 0.9609, Train Accuracy = 0.9582\n",
            "Epoch 14: Test Loss = 0.123902, Train Loss = 0.130228\n",
            "Epoch 14: Test Accuracy = 0.9620, Train Accuracy = 0.9616\n",
            "Epoch 15: Test Loss = 0.103803, Train Loss = 0.121623\n",
            "Epoch 15: Test Accuracy = 0.9671, Train Accuracy = 0.9638\n",
            "Epoch 16: Test Loss = 0.115409, Train Loss = 0.115134\n",
            "Epoch 16: Test Accuracy = 0.9634, Train Accuracy = 0.9659\n",
            "Epoch 17: Test Loss = 0.095718, Train Loss = 0.108827\n",
            "Epoch 17: Test Accuracy = 0.9704, Train Accuracy = 0.9679\n",
            "Epoch 18: Test Loss = 0.100209, Train Loss = 0.103553\n",
            "Epoch 18: Test Accuracy = 0.9678, Train Accuracy = 0.9690\n",
            "Epoch 19: Test Loss = 0.095454, Train Loss = 0.099405\n",
            "Epoch 19: Test Accuracy = 0.9702, Train Accuracy = 0.9703\n",
            "Epoch 20: Test Loss = 0.084693, Train Loss = 0.095573\n",
            "Epoch 20: Test Accuracy = 0.9723, Train Accuracy = 0.9713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IT'S NOT THE RBF?"
      ],
      "metadata": {
        "id": "z67q8esUfgeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YUP, IT'S THE MOTHERFUCKING NORMALIZATION"
      ],
      "metadata": {
        "id": "j9PyWyOTpZzv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L7o0GLBRfYMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}